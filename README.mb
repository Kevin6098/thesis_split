1. Cleansing: transform raw comments into analysable tokens.
a. Normalise text – Unicode NFKC + lowercase Latin chars.
b. Remove noise – strip URLs (https://…), email/@mentions, hashtags, HTML tags.
c. Strip non‑content characters – delete all punctuation, symbols, emoji, and any digits (0‑9, ０‑９, Japanese numerals 一二三…).
d. Tokenise – use a Japanese tokenizer (Janome/MeCab) configured so compound kanji words like 時間 stay intact (avoid per‑character splits).
e. POS filter – keep only 名詞 (nouns) & 形容詞 (adjectives); drop everything else.
f. Extra filters – discard single‑character Hiragana, the token し, and words in stop_words.py.
g. Output – join remaining tokens with spaces → clean_joined column for TF‑IDF.

2. Silhouette score: test k‑means with k = 2‑10 to find the best k.

3. Clustering analysis: cluster the reviews.

4. Topic inspection: for each cluster, drill down to the problems / claims customers raise.
a. Filter by negative sentiment – flag reviews that contain a curated list of complaint keywords (e.g. 高い, 遅い, 最悪, 残念) or run a Japanese BERT‑based sentiment model to keep only negative sentences.
b. Extract key n‑grams – build TF‑IDF or frequency counts (unigrams / bigrams) within those negative reviews to surface common pain‑points.
c. Summarise – list the top ~10 phrases per cluster; optionally generate an LLM summary sentence such as “Portion size too small and wait times long.”
d. Link back to examples – keep sample review IDs or texts so you can read raw feedback for context.
e. Visualise – word clouds or bar charts of complaint phrases per cluster help spot patterns quickly.